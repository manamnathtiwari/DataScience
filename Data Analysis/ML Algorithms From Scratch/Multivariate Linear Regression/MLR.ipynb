{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "def initialize_parameters(lenw):\n",
    "    # w = np.random.randn(1,lenw)\n",
    "    w = np.zeros((lenw,1))\n",
    "    b = 0 \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "def forward_prop(X,w,b):     # w--->1 * n  , X---> nXM\n",
    "    z = np.dot(X,w) + b # z --> 1xm b_vector = [b b b b]\n",
    "    return z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 \n",
    "def cost_function(z,y):\n",
    "    m = y.shape[1]\n",
    "    J = (1/(2*m))*np.sum(np.square(z-y))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 \n",
    "def back_prop(X,y,z):\n",
    "    m = y.shape[1]\n",
    "    dz = (1/m)*(z-y)\n",
    "    dw = np.dot(dz, X.T)\n",
    "    db = np.sum(dz)\n",
    "    return dw,db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 \n",
    "def gradient_descent_update(w,b,dw,db,learning_rate):\n",
    "    w = w - learning_rate*dw\n",
    "    b = b - learning_rate*db\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6\n",
    "def linear_regression_model(X_train,y_train,X_val,y_val,learning_rate,epochs):\n",
    "    \n",
    "    lenw = X_train.shape[0]\n",
    "    w,b = initialize_parameters(lenw)\n",
    "    \n",
    "    cost_train = []\n",
    "    m_train = y_train.shape[1]\n",
    "    m_val = y_val.shape[1]\n",
    "    \n",
    "    for i in range(1,epochs+1):\n",
    "        z_train = forward_prop(X_train,w,b)\n",
    "        cost_train = cost_function(z_train,y_train)\n",
    "        dw,db = back_prop(X_train,y_train,z_train)\n",
    "        w,b = gradient_descent_update(w,b,dw,db,learning_rate)\n",
    "        \n",
    "        # Store training cost in a list for plotting purpose\n",
    "        if i%10 == 0 :\n",
    "            cost_train.append(cost_train)\n",
    "            \n",
    "        # MAE_train \n",
    "        MAE_train = (1/m_train)*np.sum(np.abs(z_train-y_train))\n",
    "        \n",
    "        # cost_val , MAE_val\n",
    "        z_val = forward_prop(X_val,w,b)\n",
    "        cost_val = cost_function(z_val,y_val)\n",
    "        MAE_val = (1/m_val)*np.sum(np.abs(z_val-y_val))\n",
    "        \n",
    "        # Print out cost_train , cost_val , MAE_train , MAE_Val\n",
    "        print('Epochs '+ str(i) + '/'+str(epochs)+': ')\n",
    "        print('Training cost'+str(cost_train)+'|'+'Validation cost '+ str(cost_val) )\n",
    "        print('MAE cost '+str(MAE_train)+'|'+'Validation cost'+str(MAE_val))\n",
    "        \n",
    "        \n",
    "    plt.plot(cost_train)\n",
    "    plt.xlabel('Iteration(per tens)')\n",
    "    plt.ylabel('Training cost ')\n",
    "    plt.title('Learning rate'+str(learning_rate))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
